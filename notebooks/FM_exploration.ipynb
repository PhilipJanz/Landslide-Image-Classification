{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'configilm'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(Path().resolve().parent / \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FM_MODEL_DIR\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfoundation_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreben_training_scripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreben_publication\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mBigEarthNetv2_0_ImageClassifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BigEarthNetv2_0_ImageClassifier\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\Landslide-Image-Classification\\models\\foundation_models\\reben_training_scripts\\reben_publication\\BigEarthNetv2_0_ImageClassifier.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfigilm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfigILM\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfigilm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mConfigILM\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ILMConfiguration\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfigilm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mConfigILM\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ILMType\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'configilm'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add the src directory to the Python path\n",
        "sys.path.append(str(Path().resolve().parent / \"src\"))\n",
        "sys.path.append(str(Path().resolve().parent / \"models\"))\n",
        "from config import FM_MODEL_DIR\n",
        "\n",
        "from foundation_models.reben_training_scripts.reben_publication.BigEarthNetv2_0_ImageClassifier import BigEarthNetv2_0_ImageClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement configilm~=0.7.0 (from versions: 0.2.0, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.7, 0.4.8, 0.4.9, 0.4.10)\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "ERROR: No matching distribution found for configilm~=0.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install configilm[full]~=0.7.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\phili\\torch1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add the src directory to the Python path\n",
        "sys.path.append(str(Path().resolve().parent / \"src\"))\n",
        "from config import FM_MODEL_DIR\n",
        "from utils.load_fm import vit_base_patch16\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the path to your downloaded model file\n",
        "# Make sure to replace 'B13_vitb16_fgmae_ep99.pth' with the actual path if it's different.\n",
        "model_path = FM_MODEL_DIR / 'B13_vitb16_fgmae_ep99.pth'\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (patch_drop): Identity()\n",
              "  (norm_pre): Identity()\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc_norm): Identity()\n",
              "  (head_drop): Dropout(p=0.0, inplace=False)\n",
              "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 2: Load ViT-B/16 Architecture\n",
        "model = vit_base_patch16()\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Weights loaded.\n",
            "❌ Missing keys: ['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias']\n",
            "❌ Unexpected keys: ['model', 'optimizer', 'epoch', 'scaler', 'args']\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Load Checkpoint Weights\n",
        "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "# Handle possible DDP or wrapper structure\n",
        "state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
        "\n",
        "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "print(\"✅ Weights loaded.\")\n",
        "print(\"❌ Missing keys:\", missing)\n",
        "print(\"❌ Unexpected keys:\", unexpected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VisionTransformer                        [1, 1000]                 152,064\n",
              "├─PatchEmbed: 1-1                        [1, 196, 768]             --\n",
              "│    └─Conv2d: 2-1                       [1, 768, 14, 14]          590,592\n",
              "│    └─Identity: 2-2                     [1, 196, 768]             --\n",
              "├─Dropout: 1-2                           [1, 197, 768]             --\n",
              "├─Sequential: 1-3                        --                        --\n",
              "│    └─Block: 2-3                        [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-1               [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-2               [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-3                [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-4                [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-5               [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-6                     [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-7                [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-8                [1, 197, 768]             --\n",
              "│    └─Block: 2-4                        [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-9               [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-10              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-11               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-12               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-13              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-14                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-15               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-16               [1, 197, 768]             --\n",
              "│    └─Block: 2-5                        [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-17              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-18              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-19               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-20               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-21              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-22                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-23               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-24               [1, 197, 768]             --\n",
              "│    └─Block: 2-6                        [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-25              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-26              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-27               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-28               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-29              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-30                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-31               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-32               [1, 197, 768]             --\n",
              "│    └─Block: 2-7                        [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-33              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-34              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-35               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-36               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-37              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-38                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-39               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-40               [1, 197, 768]             --\n",
              "│    └─Block: 2-8                        [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-41              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-42              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-43               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-44               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-45              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-46                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-47               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-48               [1, 197, 768]             --\n",
              "│    └─Block: 2-9                        [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-49              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-50              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-51               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-52               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-53              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-54                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-55               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-56               [1, 197, 768]             --\n",
              "│    └─Block: 2-10                       [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-57              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-58              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-59               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-60               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-61              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-62                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-63               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-64               [1, 197, 768]             --\n",
              "│    └─Block: 2-11                       [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-65              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-66              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-67               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-68               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-69              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-70                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-71               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-72               [1, 197, 768]             --\n",
              "│    └─Block: 2-12                       [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-73              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-74              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-75               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-76               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-77              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-78                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-79               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-80               [1, 197, 768]             --\n",
              "│    └─Block: 2-13                       [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-81              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-82              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-83               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-84               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-85              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-86                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-87               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-88               [1, 197, 768]             --\n",
              "│    └─Block: 2-14                       [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-89              [1, 197, 768]             1,536\n",
              "│    │    └─Attention: 3-90              [1, 197, 768]             2,362,368\n",
              "│    │    └─Identity: 3-91               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-92               [1, 197, 768]             --\n",
              "│    │    └─LayerNorm: 3-93              [1, 197, 768]             1,536\n",
              "│    │    └─Mlp: 3-94                    [1, 197, 768]             4,722,432\n",
              "│    │    └─Identity: 3-95               [1, 197, 768]             --\n",
              "│    │    └─Identity: 3-96               [1, 197, 768]             --\n",
              "├─LayerNorm: 1-4                         [1, 197, 768]             1,536\n",
              "├─Identity: 1-5                          [1, 768]                  --\n",
              "├─Dropout: 1-6                           [1, 768]                  --\n",
              "├─Linear: 1-7                            [1, 1000]                 769,000\n",
              "==========================================================================================\n",
              "Total params: 86,567,656\n",
              "Trainable params: 86,567,656\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 201.58\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 162.19\n",
              "Params size (MB): 345.66\n",
              "Estimated Total Size (MB): 508.46\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Example input size for ViT: (batch_size, channels, height, width)\n",
        "summary(model, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "compile() got an unexpected keyword argument 'optimizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compile the model with Focal Loss and additional metrics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Additional metrics\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Display the model summary\u001b[39;00m\n\u001b[32m      8\u001b[39m model.summary()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phili\\torch1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:3003\u001b[39m, in \u001b[36mModule.compile\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2994\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   2995\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2996\u001b[39m \u001b[33;03m    Compile this Module's forward using :func:`torch.compile`.\u001b[39;00m\n\u001b[32m   2997\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3001\u001b[39m \u001b[33;03m    See :func:`torch.compile` for details on the arguments for this function.\u001b[39;00m\n\u001b[32m   3002\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3003\u001b[39m     \u001b[38;5;28mself\u001b[39m._compiled_call_impl = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mTypeError\u001b[39m: compile() got an unexpected keyword argument 'optimizer'"
          ]
        }
      ],
      "source": [
        "# Compile the model with Focal Loss and additional metrics\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']  # Additional metrics\n",
        ")\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
